{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Methods.HEPA.train import train as train_hepa\n",
    "from Methods.HEPA.model import HEPA\n",
    "# from Methods.SSMAugPC.model import SSMAugPC\n",
    "# from Methods.SSMAugPC.train import train as train_ssmaugpc\n",
    "from Methods.BYOL.train import train as train_byol\n",
    "from Methods.BYOL.model import BYOL\n",
    "# from Methods.DINO.train import train as train_dino\n",
    "# from Methods.DINO.model import DINO\n",
    "# from Methods.SimCLR.train import train as train_simclr\n",
    "# from Methods.SimCLR.model import SimCLR\n",
    "# from Methods.SimSiam.train import train as train_simsiam\n",
    "# from Methods.SimSiam.model import SimSiam\n",
    "# from Methods.LAugPC2.train import train as train_laugpc2\n",
    "# from Methods.LAugPC2.model import LAugPC2\n",
    "# from Methods.VQVAE.train import train as train_vae\n",
    "# from Methods.VQVAE.model import VAE\n",
    "from Methods.AE.train import train as train_ae\n",
    "from Methods.AE.model import AE\n",
    "from Methods.MAE.train import train as train_mae\n",
    "from Methods.MAE.model import MAE\n",
    "from Methods.GPAViT.train import train as train_gpa\n",
    "from Methods.GPAViT.model import GPAViT\n",
    "from Methods.GPAMAE.train import train as train_gpamae\n",
    "from Methods.GPAMAE.model import GPAMAE\n",
    "from Methods.VAE.train import train as train_vae\n",
    "from Methods.VAE.model import VAE\n",
    "from Methods.Supervised.model import Supervised\n",
    "from Methods.Supervised.train import train as train_supervised\n",
    "\n",
    "from Examples.ModelNet10.evals import linear_probing, eval_representations\n",
    "from Examples.ModelNet10.dataset import ModelNet10\n",
    "from Utils.functional import get_optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../Datasets/ModelNet10_pics/'\n",
    "train_set = ModelNet10(root, 'train', device=device)\n",
    "train_set, val_set = train_set.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    (img1, rot1, lab1), _ = train_set[i]\n",
    "    ax.imshow(img1.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    _, (img2, rot2, lab2) = train_set[i]\n",
    "    ax.imshow(img2.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m cfgs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mAE\u001b[49m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     },\n\u001b[0;32m      7\u001b[0m     {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: AE,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     },\n\u001b[0;32m     12\u001b[0m     {\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: AE,\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m     },\n\u001b[0;32m     17\u001b[0m     {\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: AE,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     },\n\u001b[0;32m     22\u001b[0m     {\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: AE,\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m     },\n\u001b[0;32m     27\u001b[0m     {\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: VAE,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m     },\n\u001b[0;32m     32\u001b[0m     {\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: VAE,\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     36\u001b[0m     },\n\u001b[0;32m     37\u001b[0m     {\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: VAE,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m     },\n\u001b[0;32m     42\u001b[0m     {\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: VAE,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m     },\n\u001b[0;32m     47\u001b[0m     {\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: VAE,\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m     },\n\u001b[0;32m     52\u001b[0m ]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m cfgs:\n\u001b[0;32m     56\u001b[0m     Model \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AE' is not defined"
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'VAE',\n",
    "        'model': VAE,\n",
    "        'save': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'VAE',\n",
    "        'model': VAE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'VAE',\n",
    "        'model': VAE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'VAE',\n",
    "        'model': VAE,\n",
    "        'save': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'VAE',\n",
    "        'model': VAE,\n",
    "        'save': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = cfg['model']\n",
    "    backbone = 'modelnet10_cnn'\n",
    "    experiment_name = cfg['name']\n",
    "    # experiment = 'mnist_byol'\n",
    "    experiment='modelnet10'\n",
    "    # log_dir = None\n",
    "    log_dir = f'Examples/ModelNet10/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = None\n",
    "    if cfg['save']:\n",
    "        save_dir = f'Examples/ModelNet10/out/models/{experiment}/{experiment_name}.pth'\n",
    "    if Model == VAE:\n",
    "        model = Model(1, 256).to(device)\n",
    "    elif Model == AE or Model == BYOL or Model == MAE:\n",
    "        model = Model(1).to(device)\n",
    "    else:\n",
    "        model = Model(1, 5).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "        \n",
    "        if isinstance(model, HEPA):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_hepa(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                stop_at=0,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                loss_fn='mse',\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, GPAViT):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_gpa(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, GPAMAE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_gpa(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            optimiser = get_optimiser(\n",
    "                model, \n",
    "                'AdamW', \n",
    "                lr=3e-5, \n",
    "                wd=0.004, \n",
    "                exclude_bias=True,\n",
    "                exclude_bn=True,\n",
    "            )\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                loss_fn='mse',\n",
    "                beta=None,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.5,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, MAE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_mae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                mask_ratio=0.75,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, Supervised):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_supervised(\n",
    "                model,\n",
    "                optimiser,\n",
    "                num_epochs=250,\n",
    "                batch_size=cfg['batch_size'],\n",
    "                subset_size=cfg['subset_size'],\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        \n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "        # Evaluate inter-neuron correlations\n",
    "        rep_metrics = eval_representations(model, flatten=False)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Encoder/test_feature_corr', rep_metrics['corr'])\n",
    "            writer.add_scalar('Encoder/test_feature_std', rep_metrics['std'])\n",
    "\n",
    "    # linear probing\n",
    "    for n in [1, 10, 100, 1000]:\n",
    "        dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "        if log_dir is not None:\n",
    "            writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "        mnist_linear_eval(model, n, writer, flatten=False, test=True)\n",
    "\n",
    "    # # Semi-supervised learning eval\n",
    "    # for n in [1, 10, 100, 1000]:\n",
    "    #     dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "    #     if log_dir is not None:\n",
    "    #         writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    #     mnist_linear_eval(model, n, writer, flatten=False, test=True, finetune=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
