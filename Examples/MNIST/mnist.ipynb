{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Methods.iGPA.model import iGPA\n",
    "from Methods.BYOL.model import BYOL\n",
    "from Methods.AE.model import AE\n",
    "from Methods.MAE.model import MAE\n",
    "from Methods.GPAViT.model import GPAViT\n",
    "from Methods.GPAMAE.model import GPAMAE\n",
    "from Methods.VAE.model import VAE\n",
    "from Methods.Supervised.model import Supervised\n",
    "\n",
    "\n",
    "from Utils.train import train\n",
    "from Utils.evals import linear_probing\n",
    "from Utils.functional import get_optimiser, aug_interact, aug_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "root = '../Datasets/'\n",
    "dataset = datasets.MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Pad(2),\n",
    "#     # transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "#     # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#     # SigmoidTransform(),\n",
    "#     # TanhTransform(),\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Pad(2),\n",
    "#     # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#     # SigmoidTransform(),\n",
    "#     # TanhTransform()\n",
    "# ])\n",
    "train_transform, val_transform = None, None\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "# fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "# for i, ax in enumerate(axes):\n",
    "#     img, label = train_set[i]\n",
    "#     angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "#     shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "#     img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "#     ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "#     ax.set_title(f\"Label: {label}\")\n",
    "#     ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# show before and after on each row\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    img, label = train_set[i+20]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    img, label = train_set[i+20]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has_teacher': False, 'aug_mode': 'none'}\n",
      "has_teacher: False, aug_mode: none, augment: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (AE, VAE, MAE, Supervised)):\n\u001b[0;32m     65\u001b[0m         train_set\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     66\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mRandomAffine(degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, translate\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m), scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m1.1\u001b[39m), shear\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     67\u001b[0m         ])\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#     if isinstance(model, iGPA):\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#         train_set.transform = transforms.Compose([\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#         ])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#             save_every=5,\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\hepa\\Utils\\train.py:143\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimiser, train_dataset, val_dataset, num_epochs, batch_size, dataset, has_teacher, aug_mode, augment, writer, save_dir, save_every, res, root, decay_lr, warmup, flat)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Training Pass\u001b[39;00m\n\u001b[0;32m    142\u001b[0m epoch_train_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(train_loader), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m    145\u001b[0m     images2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg_args in cfgs:\n",
    "\n",
    "    experiment='kwarg_test'\n",
    "    trial = cfg_args['name']\n",
    "    Model = cfg_args['model']\n",
    "    backbone = 'mnist_cnn'\n",
    "    log=True\n",
    "\n",
    "    # Logging Initialisation\n",
    "    writer=None\n",
    "    enc_log_dir = f'Examples/MNIST/out/logs/{experiment}/Encoder/{trial}/'\n",
    "    run_no = 0\n",
    "    while os.path.exists(enc_log_dir + f'/run_{run_no}'):\n",
    "        run_no += 1\n",
    "    writer = SummaryWriter(enc_log_dir + f'/run_{run_no}')\n",
    "    # remove reduction if exists\n",
    "    if os.path.exists(enc_log_dir + '/reduction.csv'):\n",
    "        os.remove(enc_log_dir + '/reduction.csv')\n",
    "\n",
    "    # Save Initialisation\n",
    "    save_dir = None\n",
    "    if cfg_args['save']:\n",
    "        save_dir = f'Examples/MNIST/out/models/{experiment}/{trial}/run_{run_no}.pth'\n",
    "\n",
    "    if Model == VAE:\n",
    "        model = Model(1, 256).to(device)\n",
    "    elif Model == AE or Model == BYOL or Model == MAE:\n",
    "        model = Model(1).to(device)\n",
    "    else:\n",
    "        model = Model(1, 5).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "\n",
    "        train_set.transform = None\n",
    "        \n",
    "        train(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=250,\n",
    "            batch_size=256,\n",
    "            dataset='mnist',\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "        \n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # # linear probing\n",
    "    # for n in [1, 10, 100, 1000]:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/Classifier-n{n}/{experiment_name}/'\n",
    "    #     run_no = 1\n",
    "    #     while os.path.exists(dest + f'classifier/run_{run_no}'):\n",
    "    #         run_no += 1\n",
    "    #     writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    #     linear_probing(model, 'mnist', root, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_set\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6496999859809875\n",
      "Best validation accuracy: 0.6551000475883484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.886199951171875\n",
      "Best validation accuracy: 0.878300130367279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9750999212265015\n",
      "Best validation accuracy: 0.9715000987052917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9834999442100525\n",
      "Best validation accuracy: 0.9843000769615173\n"
     ]
    }
   ],
   "source": [
    "model = iGPA(1, 5).to(device)\n",
    "name = 'GPA'\n",
    "experiment = 'mnist'\n",
    "save_dir = f'Examples/MNIST/out/models/{experiment}/{name}.pth'\n",
    "sd = torch.load(save_dir)\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "experiment = 'probing'\n",
    "experiment_name = 'GPA-L2'\n",
    "\n",
    "# linear probing\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "    run_no = 1\n",
    "    while os.path.exists(dest + f'classifier/run_{run_no}'):\n",
    "        run_no += 1\n",
    "    writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    linear_probing(model, 'mnist', root, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.4721), tensor(1.))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 20)\n",
    "layer = nn.LayerNorm(x.shape[1], elementwise_affine=False)\n",
    "norm1 = layer(x)\n",
    "norm2 = F.normalize(x)\n",
    "torch.allclose(norm1, norm2)\n",
    "norm1.norm(), norm2.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_dir = f'Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "torch.save(model.state_dict(), save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sd = torch.load('Examples/MNIST/out/models/mae/mae.pth')\n",
    "model.load_state_dict(sd)\n",
    "# # linear probing\n",
    "# for n in [1, 10, 100, 1000]:\n",
    "#     dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "#     if log_dir is not None:\n",
    "#         writer = SummaryWriter(dest + f'Classifier/run_{run_no}')\n",
    "#     mnist_linear_eval(model, n, writer, flatten=False, test=True)\n",
    "\n",
    "# Semi-supervised learning eval\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'Classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, writer, flatten=False, test=True, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate downstream classification accuracy\n",
    "# for n in [cfg['subset_size']]:\n",
    "for n in [1, 10, 100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1kmnist(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        n_epochs,\n",
    "        batch_size,\n",
    "):\n",
    "    model.eval()\n",
    "    classifier = nn.Linear(model.num_features, 10, bias=False).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.AdamW(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        classifier.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        train_loss = 0\n",
    "        for _, (x, label) in loop:\n",
    "            if epoch > 0:\n",
    "                loop.set_description(f'Epoch [{epoch}/{n_epochs}]')\n",
    "                loop.set_postfix(train_loss=train_losses[-1], val_loss=val_losses[-1], val_acc=val_accs[-1])\n",
    "\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                with torch.no_grad():\n",
    "                    x = model.encoder(x)\n",
    "                pred = classifier(x.detach())\n",
    "                loss = criterion(pred, label)\n",
    "\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        num_correct = 0\n",
    "        for x, label in val_loader:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                x = model.encoder(x)\n",
    "                pred = classifier(x)\n",
    "                loss = criterion(pred, label)\n",
    "            val_loss += loss.item()\n",
    "            num_correct += (pred.argmax(1) == label).sum().item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(num_correct / len(val_set) * 100)\n",
    "        \n",
    "    return train_losses, val_losses, val_accs\n",
    "c_t_losses, c_v_losses, c_v_accs = train_1kmnist(model, train_set, val_set, 100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, None, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = cfg['name']\n",
    "    # experiment = 'pc_vs_ae1'\n",
    "    experiment = 'mnist_linear_'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-5, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = 'LAugPC'\n",
    "    experiment = 'pc_vs_ae1'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True, \n",
    "        exclude_bn=True\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DAugPC):\n",
    "            train_daugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = HEPA\n",
    "# backbone = 'mnist_cnn'\n",
    "backbone='mnist_cnn'\n",
    "# experiment_name = f'{Model.__name__}-{backbone}'\n",
    "experiment_name = f'HEPA-0'\n",
    "# experiment = 'pc_vs_ae'\n",
    "experiment = 'final'\n",
    "# log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(1, 5, backbone=backbone).to(device)\n",
    "# model = Model(1, backbone).to(device)\n",
    "# model = Model(1, backbone=backbone).to(device)\n",
    "\n",
    "optimiser = get_optimiser(\n",
    "    model, \n",
    "    'AdamW', \n",
    "    lr=3e-4, \n",
    "    wd=0.004, \n",
    "    exclude_bias=True, \n",
    "    exclude_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    if isinstance(model, HEPA):\n",
    "        train_set.transform = transforms.Compose([\n",
    "        ])\n",
    "        train_hepa(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=250,\n",
    "            batch_size=256,\n",
    "            stop_at=0,\n",
    "            train_aug_scaler='none',\n",
    "            val_aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, BYOL):\n",
    "        train_byol(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            tau_0=0.996,\n",
    "            tau_e=0.999,\n",
    "            tau_T=100,\n",
    "            normalise=True,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "    # if isinstance(model, DINO):\n",
    "    #     train_dino(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=250,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         scale_temps=2.0,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimSiam):\n",
    "    #     train_simsiam(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         beta=None,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimCLR):\n",
    "    #     train_simclr(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         temperature=1.0,\n",
    "    #         augmentation=augmentation,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "    \n",
    "    # if isinstance(model, VAE):\n",
    "    #     train_vae(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=32,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_before = train_set[0][0].unsqueeze(0)\n",
    "img_after = F_v2.affine(img, angle=0, translate=(0, 0), scale=1.0, shear=0)\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "axes[0].imshow(img_before.squeeze().cpu(), cmap='gray')\n",
    "axes[0].set_title(f\"Before\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(img_after.squeeze().cpu(), cmap='gray')\n",
    "axes[1].set_title(f\"After\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    # img_pred = model.predict(img, action)\n",
    "    img_pred = model.predict(img.flatten(1), action).view(img.shape)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 1 img of each digit\n",
    "images = []\n",
    "for i in range(10):\n",
    "    while len(images) < i+1:\n",
    "        idx = torch.randint(0, len(test_set), (1,)).item()\n",
    "        if test_set.targets[idx] == i:\n",
    "            images.append(train_set[idx][0].unsqueeze(0))\n",
    "\n",
    "angles = torch.arange(-180, 180, 45).tolist()\n",
    "translate = (0,0)\n",
    "scale = 1.0\n",
    "shear = 0.0\n",
    "\n",
    "truth = {}\n",
    "pred = {}\n",
    "\n",
    "for i in range(10):\n",
    "    images_aug = []\n",
    "    img_preds = []\n",
    "    for angle in angles:\n",
    "        img_aug = F_v2.affine(images[i], angle=angle, translate=translate, scale=scale, shear=shear)\n",
    "        action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "        images_aug.append(img_aug)\n",
    "        img_preds.append(model.predict(images[i], action).view(images[i].shape))\n",
    "    \n",
    "    truth[i] = images_aug\n",
    "    pred[i] = img_preds\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(10, 8, figsize=(10,15))\n",
    "for i in range(10):\n",
    "    for j in range(8):\n",
    "        # axes[2*i, j].imshow(truth[i][j].squeeze().cpu(), cmap='gray')\n",
    "        # axes[2*i, j].axis('off')\n",
    "        # axes[2*i+1, j].imshow(pred[i][j].squeeze().cpu().detach()\n",
    "                            #   , cmap='gray')\n",
    "        # axes[2*i+1, j].axis('off')\n",
    "        axes[i, j].imshow(pred[i][j].squeeze().cpu().detach(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Examples.MNIST.mnist_linear_1k import get_mnist_subset_loaders\n",
    "train_loader, _ = get_mnist_subset_loaders(1, 10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functional import augment\n",
    "images , _ = next(iter(train_loader))\n",
    "images_aug, actions = [], []\n",
    "for image in images:\n",
    "    img_aug, action = augment(image, 0.25)\n",
    "    images_aug.append(img_aug)\n",
    "    actions.append(action)\n",
    "\n",
    "images_aug = torch.stack(images_aug)\n",
    "actions = torch.cat(actions, dim=0)\n",
    "images_aug.shape, actions.shape\n",
    "images_pred = model.predict(images, actions, 0)\n",
    "\n",
    "# visualise the images\n",
    "fig, axes = plt.subplots(5, 3, figsize=(3,5))\n",
    "for i in range(5):\n",
    "    axes[i, 0].imshow(images[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(images_aug[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 2].imshow(images_pred[i].squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[i, 2].axis('off')\n",
    "    # label 1st col as original, 2nd as augmented, 3rd as predicted\n",
    "axes[0, 0].set_title('Original', fontsize=10)\n",
    "axes[0, 1].set_title('Augmented', fontsize=10)\n",
    "axes[0, 2].set_title('Predicted', fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
