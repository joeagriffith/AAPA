{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Methods.iGPA.model import iGPA\n",
    "from Methods.BYOL.model import BYOL\n",
    "from Methods.AE.model import AE\n",
    "from Methods.MAE.model import MAE\n",
    "from Methods.GPAViT.model import GPAViT\n",
    "from Methods.GPAMAE.model import GPAMAE\n",
    "from Methods.VAE.model import VAE\n",
    "from Methods.Supervised.model import Supervised\n",
    "\n",
    "\n",
    "from Utils.train import train\n",
    "from Utils.evals import linear_probing\n",
    "from Utils.functional import get_optimiser, aug_interact, aug_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "root = '../Datasets/'\n",
    "dataset = datasets.MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Pad(2),\n",
    "#     # transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "#     # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#     # SigmoidTransform(),\n",
    "#     # TanhTransform(),\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Pad(2),\n",
    "#     # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#     # SigmoidTransform(),\n",
    "#     # TanhTransform()\n",
    "# ])\n",
    "train_transform, val_transform = None, None\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "# fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "# for i, ax in enumerate(axes):\n",
    "#     img, label = train_set[i]\n",
    "#     angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "#     shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "#     img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "#     ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "#     ax.set_title(f\"Label: {label}\")\n",
    "#     ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# show before and after on each row\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    img, label = train_set[i+20]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    img, label = train_set[i+20]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has_teacher': False, 'aug_mode': 'none'}\n",
      "has_teacher: False, aug_mode: none, augment: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (AE, VAE, MAE, Supervised)):\n\u001b[0;32m     65\u001b[0m         train_set\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     66\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mRandomAffine(degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, translate\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m), scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m1.1\u001b[39m), shear\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     67\u001b[0m         ])\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#     if isinstance(model, iGPA):\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#         train_set.transform = transforms.Compose([\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#         ])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#             save_every=5,\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\hepa\\Utils\\train.py:143\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimiser, train_dataset, val_dataset, num_epochs, batch_size, dataset, has_teacher, aug_mode, augment, writer, save_dir, save_every, res, root, decay_lr, warmup, flat)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Training Pass\u001b[39;00m\n\u001b[0;32m    142\u001b[0m epoch_train_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(train_loader), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m    145\u001b[0m     images2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'AE',\n",
    "        'model': AE,\n",
    "        'save': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg_args in cfgs:\n",
    "\n",
    "    experiment='kwarg_test'\n",
    "    trial = cfg_args['name']\n",
    "    Model = cfg_args['model']\n",
    "    backbone = 'mnist_cnn'\n",
    "    log=True\n",
    "\n",
    "    # Logging Initialisation\n",
    "    writer=None\n",
    "    enc_log_dir = f'Examples/MNIST/out/logs/{experiment}/Encoder/{trial}/'\n",
    "    run_no = 0\n",
    "    while os.path.exists(enc_log_dir + f'/run_{run_no}'):\n",
    "        run_no += 1\n",
    "    writer = SummaryWriter(enc_log_dir + f'/run_{run_no}')\n",
    "    # remove reduction if exists\n",
    "    if os.path.exists(enc_log_dir + '/reduction.csv'):\n",
    "        os.remove(enc_log_dir + '/reduction.csv')\n",
    "\n",
    "    # Save Initialisation\n",
    "    save_dir = None\n",
    "    if cfg_args['save']:\n",
    "        save_dir = f'Examples/MNIST/out/models/{experiment}/{trial}/run_{run_no}.pth'\n",
    "\n",
    "    if Model == VAE:\n",
    "        model = Model(1, 256).to(device)\n",
    "    elif Model == AE or Model == BYOL or Model == MAE:\n",
    "        model = Model(1).to(device)\n",
    "    else:\n",
    "        model = Model(1, 5).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "\n",
    "        train_set.transform = None\n",
    "        \n",
    "        train(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=250,\n",
    "            batch_size=256,\n",
    "            dataset='mnist',\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "        \n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # # linear probing\n",
    "    # for n in [1, 10, 100, 1000]:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/Classifier-n{n}/{experiment_name}/'\n",
    "    #     run_no = 1\n",
    "    #     while os.path.exists(dest + f'classifier/run_{run_no}'):\n",
    "    #         run_no += 1\n",
    "    #     writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    #     linear_probing(model, 'mnist', root, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6496999859809875\n",
      "Best validation accuracy: 0.6551000475883484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.886199951171875\n",
      "Best validation accuracy: 0.878300130367279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9750999212265015\n",
      "Best validation accuracy: 0.9715000987052917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9834999442100525\n",
      "Best validation accuracy: 0.9843000769615173\n"
     ]
    }
   ],
   "source": [
    "model = iGPA(1, 5).to(device)\n",
    "name = 'GPA'\n",
    "experiment = 'mnist'\n",
    "save_dir = f'Examples/MNIST/out/models/{experiment}/{name}.pth'\n",
    "sd = torch.load(save_dir)\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "experiment = 'probing'\n",
    "experiment_name = 'GPA-L2'\n",
    "\n",
    "# linear probing\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "    run_no = 1\n",
    "    while os.path.exists(dest + f'classifier/run_{run_no}'):\n",
    "        run_no += 1\n",
    "    writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    linear_probing(model, 'mnist', root, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.4721), tensor(1.))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 20)\n",
    "layer = nn.LayerNorm(x.shape[1], elementwise_affine=False)\n",
    "norm1 = layer(x)\n",
    "norm2 = F.normalize(x)\n",
    "torch.allclose(norm1, norm2)\n",
    "norm1.norm(), norm2.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_dir = f'Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "torch.save(model.state_dict(), save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sd = torch.load('Examples/MNIST/out/models/mae/mae.pth')\n",
    "model.load_state_dict(sd)\n",
    "# # linear probing\n",
    "# for n in [1, 10, 100, 1000]:\n",
    "#     dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "#     if log_dir is not None:\n",
    "#         writer = SummaryWriter(dest + f'Classifier/run_{run_no}')\n",
    "#     mnist_linear_eval(model, n, writer, flatten=False, test=True)\n",
    "\n",
    "# Semi-supervised learning eval\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    dest = f'Examples/MNIST/out/logs/n{n}-{experiment}/{experiment_name}/'\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'Classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, writer, flatten=False, test=True, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate downstream classification accuracy\n",
    "# for n in [cfg['subset_size']]:\n",
    "for n in [1, 10, 100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1kmnist(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        n_epochs,\n",
    "        batch_size,\n",
    "):\n",
    "    model.eval()\n",
    "    classifier = nn.Linear(model.num_features, 10, bias=False).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.AdamW(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        classifier.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        train_loss = 0\n",
    "        for _, (x, label) in loop:\n",
    "            if epoch > 0:\n",
    "                loop.set_description(f'Epoch [{epoch}/{n_epochs}]')\n",
    "                loop.set_postfix(train_loss=train_losses[-1], val_loss=val_losses[-1], val_acc=val_accs[-1])\n",
    "\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                with torch.no_grad():\n",
    "                    x = model.encoder(x)\n",
    "                pred = classifier(x.detach())\n",
    "                loss = criterion(pred, label)\n",
    "\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        num_correct = 0\n",
    "        for x, label in val_loader:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                x = model.encoder(x)\n",
    "                pred = classifier(x)\n",
    "                loss = criterion(pred, label)\n",
    "            val_loss += loss.item()\n",
    "            num_correct += (pred.argmax(1) == label).sum().item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(num_correct / len(val_set) * 100)\n",
    "        \n",
    "    return train_losses, val_losses, val_accs\n",
    "c_t_losses, c_v_losses, c_v_accs = train_1kmnist(model, train_set, val_set, 100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, None, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = cfg['name']\n",
    "    # experiment = 'pc_vs_ae1'\n",
    "    experiment = 'mnist_linear_'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-5, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = 'LAugPC'\n",
    "    experiment = 'pc_vs_ae1'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True, \n",
    "        exclude_bn=True\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DAugPC):\n",
    "            train_daugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = HEPA\n",
    "# backbone = 'mnist_cnn'\n",
    "backbone='mnist_cnn'\n",
    "# experiment_name = f'{Model.__name__}-{backbone}'\n",
    "experiment_name = f'HEPA-0'\n",
    "# experiment = 'pc_vs_ae'\n",
    "experiment = 'final'\n",
    "# log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(1, 5, backbone=backbone).to(device)\n",
    "# model = Model(1, backbone).to(device)\n",
    "# model = Model(1, backbone=backbone).to(device)\n",
    "\n",
    "optimiser = get_optimiser(\n",
    "    model, \n",
    "    'AdamW', \n",
    "    lr=3e-4, \n",
    "    wd=0.004, \n",
    "    exclude_bias=True, \n",
    "    exclude_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    if isinstance(model, HEPA):\n",
    "        train_set.transform = transforms.Compose([\n",
    "        ])\n",
    "        train_hepa(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=250,\n",
    "            batch_size=256,\n",
    "            stop_at=0,\n",
    "            train_aug_scaler='none',\n",
    "            val_aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, BYOL):\n",
    "        train_byol(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            tau_0=0.996,\n",
    "            tau_e=0.999,\n",
    "            tau_T=100,\n",
    "            normalise=True,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "    # if isinstance(model, DINO):\n",
    "    #     train_dino(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=250,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         scale_temps=2.0,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimSiam):\n",
    "    #     train_simsiam(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         beta=None,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimCLR):\n",
    "    #     train_simclr(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         temperature=1.0,\n",
    "    #         augmentation=augmentation,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "    \n",
    "    # if isinstance(model, VAE):\n",
    "    #     train_vae(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=32,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_before = train_set[0][0].unsqueeze(0)\n",
    "img_after = F_v2.affine(img, angle=0, translate=(0, 0), scale=1.0, shear=0)\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "axes[0].imshow(img_before.squeeze().cpu(), cmap='gray')\n",
    "axes[0].set_title(f\"Before\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(img_after.squeeze().cpu(), cmap='gray')\n",
    "axes[1].set_title(f\"After\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    # img_pred = model.predict(img, action)\n",
    "    img_pred = model.predict(img.flatten(1), action).view(img.shape)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 1 img of each digit\n",
    "images = []\n",
    "for i in range(10):\n",
    "    while len(images) < i+1:\n",
    "        idx = torch.randint(0, len(test_set), (1,)).item()\n",
    "        if test_set.targets[idx] == i:\n",
    "            images.append(train_set[idx][0].unsqueeze(0))\n",
    "\n",
    "angles = torch.arange(-180, 180, 45).tolist()\n",
    "translate = (0,0)\n",
    "scale = 1.0\n",
    "shear = 0.0\n",
    "\n",
    "truth = {}\n",
    "pred = {}\n",
    "\n",
    "for i in range(10):\n",
    "    images_aug = []\n",
    "    img_preds = []\n",
    "    for angle in angles:\n",
    "        img_aug = F_v2.affine(images[i], angle=angle, translate=translate, scale=scale, shear=shear)\n",
    "        action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "        images_aug.append(img_aug)\n",
    "        img_preds.append(model.predict(images[i], action).view(images[i].shape))\n",
    "    \n",
    "    truth[i] = images_aug\n",
    "    pred[i] = img_preds\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(10, 8, figsize=(10,15))\n",
    "for i in range(10):\n",
    "    for j in range(8):\n",
    "        # axes[2*i, j].imshow(truth[i][j].squeeze().cpu(), cmap='gray')\n",
    "        # axes[2*i, j].axis('off')\n",
    "        # axes[2*i+1, j].imshow(pred[i][j].squeeze().cpu().detach()\n",
    "                            #   , cmap='gray')\n",
    "        # axes[2*i+1, j].axis('off')\n",
    "        axes[i, j].imshow(pred[i][j].squeeze().cpu().detach(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Examples.MNIST.mnist_linear_1k import get_mnist_subset_loaders\n",
    "train_loader, _ = get_mnist_subset_loaders(1, 10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functional import augment\n",
    "images , _ = next(iter(train_loader))\n",
    "images_aug, actions = [], []\n",
    "for image in images:\n",
    "    img_aug, action = augment(image, 0.25)\n",
    "    images_aug.append(img_aug)\n",
    "    actions.append(action)\n",
    "\n",
    "images_aug = torch.stack(images_aug)\n",
    "actions = torch.cat(actions, dim=0)\n",
    "images_aug.shape, actions.shape\n",
    "images_pred = model.predict(images, actions, 0)\n",
    "\n",
    "# visualise the images\n",
    "fig, axes = plt.subplots(5, 3, figsize=(3,5))\n",
    "for i in range(5):\n",
    "    axes[i, 0].imshow(images[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(images_aug[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 2].imshow(images_pred[i].squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[i, 2].axis('off')\n",
    "    # label 1st col as original, 2nd as augmented, 3rd as predicted\n",
    "axes[0, 0].set_title('Original', fontsize=10)\n",
    "axes[0, 1].set_title('Augmented', fontsize=10)\n",
    "axes[0, 2].set_title('Predicted', fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
