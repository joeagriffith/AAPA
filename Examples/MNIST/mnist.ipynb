{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Methods.HEPA.train import train as train_hepa\n",
    "from Methods.HEPA.model import HEPA\n",
    "# from Methods.SSMAugPC.model import SSMAugPC\n",
    "# from Methods.SSMAugPC.train import train as train_ssmaugpc\n",
    "from Methods.BYOL.train import train as train_byol\n",
    "from Methods.BYOL.model import BYOL\n",
    "# from Methods.DINO.train import train as train_dino\n",
    "# from Methods.DINO.model import DINO\n",
    "# from Methods.SimCLR.train import train as train_simclr\n",
    "# from Methods.SimCLR.model import SimCLR\n",
    "# from Methods.SimSiam.train import train as train_simsiam\n",
    "# from Methods.SimSiam.model import SimSiam\n",
    "# from Methods.LAugPC2.train import train as train_laugpc2\n",
    "# from Methods.LAugPC2.model import LAugPC2\n",
    "# from Methods.VQVAE.train import train as train_vae\n",
    "# from Methods.VQVAE.model import VAE\n",
    "from Methods.AE.train import train as train_ae\n",
    "from Methods.AE.model import AE\n",
    "from Methods.VAE.train import train as train_vae\n",
    "from Methods.VAE.model import VAE\n",
    "from Methods.Supervised.model import Supervised\n",
    "from Methods.Supervised.train import train as train_supervised\n",
    "\n",
    "from Examples.MNIST.mnist_linear_1k import mnist_linear_eval, eval_representations\n",
    "from Utils.functional import get_optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Pad(2),\n",
    "    # transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Pad(2),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "# fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "# for i, ax in enumerate(axes):\n",
    "#     img, label = train_set[i]\n",
    "#     angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "#     scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "#     shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "#     img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "#     ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "#     ax.set_title(f\"Label: {label}\")\n",
    "#     ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# show before and after on each row\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15,5))\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    img, label = train_set[i+20]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    img, label = train_set[i+20]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6704000234603882\n",
      "Best validation accuracy: 0.6685000658035278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8944000005722046\n",
      "Best validation accuracy: 0.8856000900268555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.973800003528595\n",
      "Best validation accuracy: 0.9721000790596008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9857999086380005\n",
      "Best validation accuracy: 0.9883000254631042\n"
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'HEPA-0',\n",
    "    },\n",
    "    {\n",
    "        'name': 'HEPA-0',\n",
    "    },\n",
    "    {\n",
    "        'name': 'HEPA-0',\n",
    "    },\n",
    "    {\n",
    "        'name': 'HEPA-0',\n",
    "    },\n",
    "    {\n",
    "        'name': 'HEPA-0',\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = HEPA\n",
    "    backbone = 'mnist_cnn'\n",
    "    experiment_name = cfg['name']\n",
    "    # experiment = 'mnist_byol'\n",
    "    experiment='HEPA-augs'\n",
    "    log_dir = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    # log_dir = None\n",
    "    save_dir = None\n",
    "    if Model == VAE:\n",
    "        model = Model(1, 256).to(device)\n",
    "    elif Model == AE or Model == BYOL:\n",
    "        model = Model(1).to(device)\n",
    "    else:\n",
    "        model = Model(1, 5).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "        \n",
    "        if isinstance(model, HEPA):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_hepa(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                stop_at=0,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                loss_fn='mse',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=cfg['beta'],\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_set.transform = transforms.Compose([\n",
    "            ])\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                loss_fn='mse',\n",
    "                beta=None,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, Supervised):\n",
    "            train_set.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "            ])\n",
    "            train_supervised(\n",
    "                model,\n",
    "                optimiser,\n",
    "                num_epochs=250,\n",
    "                batch_size=cfg['batch_size'],\n",
    "                subset_size=cfg['subset_size'],\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        \n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "        # Evaluate inter-neuron correlations\n",
    "        rep_metrics = eval_representations(model, flatten=False)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Encoder/test_feature_corr', rep_metrics['corr'])\n",
    "            writer.add_scalar('Encoder/test_feature_std', rep_metrics['std'])\n",
    "\n",
    "    # Evaluate downstream classification accuracy\n",
    "    # for n in [cfg['subset_size']]:\n",
    "    for n in [1, 10, 100, 1000]:\n",
    "    # for n in [100]:\n",
    "        # try:\n",
    "        #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "        #     shutil.copytree(log_dir, dest)\n",
    "        # except:\n",
    "        #     pass\n",
    "        dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "        # dest = log_dir\n",
    "        if log_dir is not None:\n",
    "            writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "        mnist_linear_eval(model, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6703999042510986\n",
      "Best validation accuracy: 0.6752001047134399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8902999758720398\n",
      "Best validation accuracy: 0.884100079536438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.968299925327301\n",
      "Best validation accuracy: 0.9690000414848328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9814999103546143\n",
      "Best validation accuracy: 0.9837000966072083\n"
     ]
    }
   ],
   "source": [
    "# Evaluate downstream classification accuracy\n",
    "# for n in [cfg['subset_size']]:\n",
    "for n in [1, 10, 100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, writer, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "def train_1kmnist(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        n_epochs,\n",
    "        batch_size,\n",
    "):\n",
    "    model.eval()\n",
    "    classifier = nn.Linear(model.num_features, 10, bias=False).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.AdamW(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        classifier.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        train_loss = 0\n",
    "        for _, (x, label) in loop:\n",
    "            if epoch > 0:\n",
    "                loop.set_description(f'Epoch [{epoch}/{n_epochs}]')\n",
    "                loop.set_postfix(train_loss=train_losses[-1], val_loss=val_losses[-1], val_acc=val_accs[-1])\n",
    "\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                with torch.no_grad():\n",
    "                    x = model.encoder(x)\n",
    "                pred = classifier(x.detach())\n",
    "                loss = criterion(pred, label)\n",
    "\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        num_correct = 0\n",
    "        for x, label in val_loader:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                x = model.encoder(x)\n",
    "                pred = classifier(x)\n",
    "                loss = criterion(pred, label)\n",
    "            val_loss += loss.item()\n",
    "            num_correct += (pred.argmax(1) == label).sum().item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(num_correct / len(val_set) * 100)\n",
    "        \n",
    "    return train_losses, val_losses, val_accs\n",
    "c_t_losses, c_v_losses, c_v_accs = train_1kmnist(model, train_set, val_set, 100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8975999355316162\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'add_scalar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     writer \u001b[38;5;241m=\u001b[39m SummaryWriter(dest \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmnist_linear_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\hepa\\Examples\\MNIST\\mnist_linear_1k.py:148\u001b[0m, in \u001b[0;36mmnist_linear_eval\u001b[1;34m(model, n_per_class, writer, flatten, test)\u001b[0m\n\u001b[0;32m    146\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test_accs\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassifier/test_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_acc\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add_scalar'"
     ]
    }
   ],
   "source": [
    "for n in [100, 1000]:\n",
    "# for n in [100]:\n",
    "    # try:\n",
    "    #     dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    #     shutil.copytree(log_dir, dest)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # dest = f'Examples/MNIST/out/logs/{experiment}/{experiment_name}-n{n}/'\n",
    "    # dest = log_dir\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(dest + f'classifier/run_{run_no}')\n",
    "    mnist_linear_eval(model, n, None, flatten=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "    {\n",
    "        'name': 'proj-3e-5-mse',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = cfg['name']\n",
    "    # experiment = 'pc_vs_ae1'\n",
    "    experiment = 'mnist_linear_'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-5, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True,\n",
    "        exclude_bn=True,\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    },\n",
    "    {\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "\n",
    "    Model = LAugPC\n",
    "    # backbone = 'mnist_cnn'\n",
    "    backbone='mnist_cnn'\n",
    "    experiment_name = 'LAugPC'\n",
    "    experiment = 'pc_vs_ae1'\n",
    "    log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "    save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "    save_dir = None\n",
    "    model = Model(1, 5,\n",
    "                backbone=backbone, \n",
    "                ).to(device)\n",
    "\n",
    "    optimiser = get_optimiser(\n",
    "        model, \n",
    "        'AdamW', \n",
    "        # lr = cfg['lr'],\n",
    "        lr=3e-4, \n",
    "        wd=0.004, \n",
    "        exclude_bias=True, \n",
    "        exclude_bn=True\n",
    "    )\n",
    "\n",
    "    to_train = True\n",
    "    if save_dir is not None:\n",
    "        try:\n",
    "            sd = torch.load(save_dir)\n",
    "            # change keys \"project\" to \"transition\"\n",
    "            for key in list(sd.keys()):\n",
    "                if 'project' in key:\n",
    "                    sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "            model.load_state_dict(sd)\n",
    "            to_train = False\n",
    "            print('Model loaded successfully')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "            print('Model not found, training new model')\n",
    "    if to_train:\n",
    "        writer = None\n",
    "        if log_dir is not None:\n",
    "            # remove reduction if exists\n",
    "            if os.path.exists(log_dir + 'encoder/reduction.csv'):\n",
    "                os.remove(log_dir + 'encoder/reduction.csv')\n",
    "            if os.path.exists(log_dir + 'classifier/reduction.csv'):\n",
    "                os.remove(log_dir + 'classifier/reduction.csv')\n",
    "\n",
    "            run_no = 1\n",
    "            while os.path.exists(log_dir + 'encoder/' + f'run_{run_no}'):\n",
    "                run_no += 1\n",
    "            writer = SummaryWriter(log_dir + 'encoder/' + f'run_{run_no}')\n",
    "\n",
    "        if isinstance(model, LAugPC):\n",
    "            train_laugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "\n",
    "        if isinstance(model, AugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, SSMAugPC):\n",
    "            train_augpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DAugPC):\n",
    "            train_daugpc(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                train_aug_scaler='none',\n",
    "                val_aug_scaler='none',\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, BYOL):\n",
    "            train_byol(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, VAE):\n",
    "            train_vae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=0.75,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, AE):\n",
    "            train_ae(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                beta=None,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        if isinstance(model, DINO):\n",
    "            train_dino(\n",
    "                model,\n",
    "                optimiser,\n",
    "                train_set,\n",
    "                val_set,\n",
    "                num_epochs=250,\n",
    "                batch_size=256,\n",
    "                augmentation=augmentation,\n",
    "                scale_temps=2.0,\n",
    "                learn_on_ss=False,\n",
    "                writer=writer,\n",
    "                save_dir=save_dir,\n",
    "                save_every=5,\n",
    "            )\n",
    "        print(f'Finished training')\n",
    "        if save_dir is not None:\n",
    "            print('Run cell again to load best (val_acc) model.')\n",
    "\n",
    "    # collect 100 of each target index from train_set.targets\n",
    "    writer = SummaryWriter(log_dir + f'classifier/run_{run_no}')\n",
    "    mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = HEPA\n",
    "# backbone = 'mnist_cnn'\n",
    "backbone='mnist_cnn'\n",
    "# experiment_name = f'{Model.__name__}-{backbone}'\n",
    "experiment_name = f'HEPA-0'\n",
    "# experiment = 'pc_vs_ae'\n",
    "experiment = 'final'\n",
    "# log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{experiment}/{experiment_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{experiment}/{experiment_name}.pth'\n",
    "log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(1, 5, backbone=backbone).to(device)\n",
    "# model = Model(1, backbone).to(device)\n",
    "# model = Model(1, backbone=backbone).to(device)\n",
    "\n",
    "optimiser = get_optimiser(\n",
    "    model, \n",
    "    'AdamW', \n",
    "    lr=3e-4, \n",
    "    wd=0.004, \n",
    "    exclude_bias=True, \n",
    "    exclude_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    if isinstance(model, HEPA):\n",
    "        train_set.transform = transforms.Compose([\n",
    "        ])\n",
    "        train_hepa(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=250,\n",
    "            batch_size=256,\n",
    "            stop_at=0,\n",
    "            train_aug_scaler='none',\n",
    "            val_aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, BYOL):\n",
    "        train_byol(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            tau_0=0.996,\n",
    "            tau_e=0.999,\n",
    "            tau_T=100,\n",
    "            normalise=True,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "    # if isinstance(model, DINO):\n",
    "    #     train_dino(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=250,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         scale_temps=2.0,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimSiam):\n",
    "    #     train_simsiam(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         augmentation=augmentation,\n",
    "    #         beta=None,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    # if isinstance(model, SimCLR):\n",
    "    #     train_simclr(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=256,\n",
    "    #         temperature=1.0,\n",
    "    #         augmentation=augmentation,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "    \n",
    "    # if isinstance(model, VAE):\n",
    "    #     train_vae(\n",
    "    #         model,\n",
    "    #         optimiser,\n",
    "    #         train_set,\n",
    "    #         val_set,\n",
    "    #         num_epochs=500,\n",
    "    #         batch_size=32,\n",
    "    #         learn_on_ss=False,\n",
    "    #         writer=writer,\n",
    "    #         save_dir=save_dir,\n",
    "    #         save_every=5,\n",
    "    #     )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_before = train_set[0][0].unsqueeze(0)\n",
    "img_after = F_v2.affine(img, angle=0, translate=(0, 0), scale=1.0, shear=0)\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "axes[0].imshow(img_before.squeeze().cpu(), cmap='gray')\n",
    "axes[0].set_title(f\"Before\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(img_after.squeeze().cpu(), cmap='gray')\n",
    "axes[1].set_title(f\"After\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    # img_pred = model.predict(img, action)\n",
    "    img_pred = model.predict(img.flatten(1), action).view(img.shape)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 1 img of each digit\n",
    "images = []\n",
    "for i in range(10):\n",
    "    while len(images) < i+1:\n",
    "        idx = torch.randint(0, len(test_set), (1,)).item()\n",
    "        if test_set.targets[idx] == i:\n",
    "            images.append(train_set[idx][0].unsqueeze(0))\n",
    "\n",
    "angles = torch.arange(-180, 180, 45).tolist()\n",
    "translate = (0,0)\n",
    "scale = 1.0\n",
    "shear = 0.0\n",
    "\n",
    "truth = {}\n",
    "pred = {}\n",
    "\n",
    "for i in range(10):\n",
    "    images_aug = []\n",
    "    img_preds = []\n",
    "    for angle in angles:\n",
    "        img_aug = F_v2.affine(images[i], angle=angle, translate=translate, scale=scale, shear=shear)\n",
    "        action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "        images_aug.append(img_aug)\n",
    "        img_preds.append(model.predict(images[i], action).view(images[i].shape))\n",
    "    \n",
    "    truth[i] = images_aug\n",
    "    pred[i] = img_preds\n",
    "\n",
    "# Show example images\n",
    "fig, axes = plt.subplots(10, 8, figsize=(10,15))\n",
    "for i in range(10):\n",
    "    for j in range(8):\n",
    "        # axes[2*i, j].imshow(truth[i][j].squeeze().cpu(), cmap='gray')\n",
    "        # axes[2*i, j].axis('off')\n",
    "        # axes[2*i+1, j].imshow(pred[i][j].squeeze().cpu().detach()\n",
    "                            #   , cmap='gray')\n",
    "        # axes[2*i+1, j].axis('off')\n",
    "        axes[i, j].imshow(pred[i][j].squeeze().cpu().detach(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Examples.MNIST.mnist_linear_1k import get_mnist_subset_loaders\n",
    "train_loader, _ = get_mnist_subset_loaders(1, 10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functional import augment\n",
    "images , _ = next(iter(train_loader))\n",
    "images_aug, actions = [], []\n",
    "for image in images:\n",
    "    img_aug, action = augment(image, 0.25)\n",
    "    images_aug.append(img_aug)\n",
    "    actions.append(action)\n",
    "\n",
    "images_aug = torch.stack(images_aug)\n",
    "actions = torch.cat(actions, dim=0)\n",
    "images_aug.shape, actions.shape\n",
    "images_pred = model.predict(images, actions, 0)\n",
    "\n",
    "# visualise the images\n",
    "fig, axes = plt.subplots(5, 3, figsize=(3,5))\n",
    "for i in range(5):\n",
    "    axes[i, 0].imshow(images[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(images_aug[i].squeeze().cpu(), cmap='gray')\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 2].imshow(images_pred[i].squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[i, 2].axis('off')\n",
    "    # label 1st col as original, 2nd as augmented, 3rd as predicted\n",
    "axes[0, 0].set_title('Original', fontsize=10)\n",
    "axes[0, 1].set_title('Augmented', fontsize=10)\n",
    "axes[0, 2].set_title('Predicted', fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
